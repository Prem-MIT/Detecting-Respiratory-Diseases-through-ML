{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7884a978",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import re\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = '/content/ICBHI_dataset'\n",
    "DIAGNOSIS_CSV = '/content/patient_diagnosis.csv'\n",
    "\n",
    "# Set parameters for audio processing\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 5  # seconds\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Create a class mapping for diagnosis\n",
    "class_mapping = {\n",
    "    'Healthy': 0,\n",
    "    'COPD': 1,\n",
    "    'Asthma': 1,\n",
    "    'Bronchiectasis': 1,\n",
    "    'Bronchiolitis': 1,\n",
    "    'LRTI': 2,\n",
    "    'Pneumonia': 2,\n",
    "    'URTI': 3\n",
    "}\n",
    "\n",
    "def load_diagnosis_data():\n",
    "    \"\"\"Load patient diagnosis data from CSV file\"\"\"\n",
    "    try:\n",
    "        diagnosis_df = pd.read_csv(DIAGNOSIS_CSV, header=None, names=['patient_id', 'diagnosis'])\n",
    "        print(f\"Loaded diagnosis data for {len(diagnosis_df)} patients\")\n",
    "\n",
    "        print(\"\\nDiagnosis distribution in CSV:\")\n",
    "        diagnosis_counts = diagnosis_df['diagnosis'].value_counts()\n",
    "        print(diagnosis_counts)\n",
    "\n",
    "        diagnosis_dict = dict(zip(diagnosis_df['patient_id'].astype(str), diagnosis_df['diagnosis']))\n",
    "\n",
    "        return diagnosis_dict\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading diagnosis data: {e}\")\n",
    "        return None\n",
    "\n",
    "def find_wav_files():\n",
    "    \"\"\"Find all WAV files in the dataset directory\"\"\"\n",
    "    wav_files = glob.glob(os.path.join(DATASET_PATH, '**', '*.wav'), recursive=True)\n",
    "    print(f\"Found {len(wav_files)} WAV files\")\n",
    "    return wav_files\n",
    "\n",
    "def extract_patient_id_from_filename(filename):\n",
    "    \"\"\"Extract patient ID from WAV filename\"\"\"\n",
    "    # Common patterns for ICBHI dataset filenames\n",
    "    patterns = [\n",
    "        # Try to extract a numeric ID that matches those in the CSV\n",
    "        r'(\\d{3})_',  # Format: 101_something.wav\n",
    "        r'p(\\d{3})',  # Format: p101_something.wav\n",
    "        r'patient(\\d{3})',  # Format: patient101_something.wav\n",
    "        r'_(\\d{3})\\.wav'  # Format: something_101.wav\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, os.path.basename(filename))\n",
    "        if match:\n",
    "            return match.group(1)\n",
    "\n",
    "    # If no pattern matches, try to get the first 3 digits from the filename\n",
    "    digits = re.findall(r'\\d{3}', os.path.basename(filename))\n",
    "    if digits:\n",
    "        return digits[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "def build_dataset(wav_files, diagnosis_dict):\n",
    "    \"\"\"Build dataset from WAV files and diagnosis dictionary\"\"\"\n",
    "    annotations = []\n",
    "    missing_diagnosis = []\n",
    "\n",
    "    print(\"Matching WAV files with diagnosis data...\")\n",
    "    for wav_file in tqdm(wav_files):\n",
    "        # Extract patient ID from the filename\n",
    "        patient_id = extract_patient_id_from_filename(wav_file)\n",
    "\n",
    "        if patient_id and patient_id in diagnosis_dict:\n",
    "            diagnosis = diagnosis_dict[patient_id]\n",
    "\n",
    "            # Check if the diagnosis is in our class mapping\n",
    "            if diagnosis in class_mapping:\n",
    "                annotations.append({\n",
    "                    'filename': wav_file,\n",
    "                    'patient_id': patient_id,\n",
    "                    'diagnosis': diagnosis,\n",
    "                    'diagnosis_class': class_mapping[diagnosis]\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Warning: Unrecognized diagnosis '{diagnosis}' for patient {patient_id}\")\n",
    "        else:\n",
    "            missing_diagnosis.append(wav_file)\n",
    "\n",
    "    if missing_diagnosis:\n",
    "        print(f\"\\n{len(missing_diagnosis)} WAV files could not be matched with a diagnosis.\")\n",
    "        print(\"First few unmatched files:\")\n",
    "        for file in missing_diagnosis[:5]:\n",
    "            print(f\"  - {os.path.basename(file)}\")\n",
    "\n",
    "    if not annotations:\n",
    "        print(\"\\nNo files could be matched with diagnosis information.\")\n",
    "        return None\n",
    "\n",
    "    result_df = pd.DataFrame(annotations)\n",
    "\n",
    "    print(f\"\\nSuccessfully matched {len(result_df)} WAV files with diagnosis information\")\n",
    "    print(\"\\nDiagnosis distribution in dataset:\")\n",
    "    diagnosis_counts = result_df['diagnosis'].value_counts()\n",
    "    print(diagnosis_counts)\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def extract_features(file_path):\n",
    "    \"\"\"Extract Mel spectrogram features from an audio file\"\"\"\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        # If audio is too long, take a segment\n",
    "        if len(y) > SAMPLE_RATE * DURATION:\n",
    "            y = y[:int(SAMPLE_RATE * DURATION)]\n",
    "        else:\n",
    "            # Pad with zeros if audio is too short\n",
    "            pad_length = int(SAMPLE_RATE * DURATION) - len(y)\n",
    "            if pad_length > 0:\n",
    "                y = np.pad(y, (0, pad_length), mode='constant')\n",
    "\n",
    "        # Generate Mel spectrogram\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        # Normalize\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def prepare_dataset(annotations_df):\n",
    "    \"\"\"Prepare dataset for training by extracting features and labels\"\"\"\n",
    "    if annotations_df is None or len(annotations_df) == 0:\n",
    "        raise ValueError(\"No valid annotations found\")\n",
    "\n",
    "    X = []\n",
    "    y = []\n",
    "    filenames = []\n",
    "    patient_ids = []\n",
    "\n",
    "    for idx, row in tqdm(annotations_df.iterrows(), total=len(annotations_df), desc=\"Processing audio\"):\n",
    "        file_path = row['filename']\n",
    "        if os.path.exists(file_path):\n",
    "            features = extract_features(file_path)\n",
    "            if features is not None:\n",
    "                X.append(features)\n",
    "                y.append(row['diagnosis_class'])\n",
    "                filenames.append(file_path)\n",
    "                patient_ids.append(row['patient_id'])\n",
    "\n",
    "    if not X or not y:\n",
    "        raise ValueError(\"No valid features extracted\")\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # Reshape for CNN input (samples, height, width, channels)\n",
    "    X = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n",
    "\n",
    "    # Convert labels to categorical\n",
    "    num_classes = len(np.unique(y))\n",
    "    y = to_categorical(y, num_classes=num_classes)\n",
    "\n",
    "    return X, y, filenames, patient_ids\n",
    "\n",
    "def ensure_patient_stratification(annotations_df):\n",
    "    \"\"\"\n",
    "    Ensure that files from the same patient don't appear in both training and test sets\n",
    "    by creating patient-based splits with handling for rare diagnoses\n",
    "    \"\"\"\n",
    "    # Group by patient_id and diagnosis\n",
    "    patient_groups = annotations_df.groupby(['patient_id', 'diagnosis']).size().reset_index(name='count')\n",
    "\n",
    "    # Get diagnosis counts per patient\n",
    "    diagnosis_counts = patient_groups.groupby('diagnosis').size()\n",
    "\n",
    "    # Identify rare diagnoses (those with only 1 patient)\n",
    "    rare_diagnoses = diagnosis_counts[diagnosis_counts < 2].index.tolist()\n",
    "    print(f\"Rare diagnoses with only one patient: {rare_diagnoses}\")\n",
    "\n",
    "    # For patients with rare diagnoses, we'll handle them separately\n",
    "    common_patients = patient_groups[~patient_groups['diagnosis'].isin(rare_diagnoses)]['patient_id'].unique()\n",
    "    rare_patients = patient_groups[patient_groups['diagnosis'].isin(rare_diagnoses)]['patient_id'].unique()\n",
    "\n",
    "    # If we have patients with common diagnoses, do a stratified split on those\n",
    "    if len(common_patients) > 0:\n",
    "        common_patient_diagnoses = patient_groups[~patient_groups['diagnosis'].isin(rare_diagnoses)]\n",
    "        patient_train, patient_test = train_test_split(\n",
    "            common_patients,\n",
    "            test_size=0.2,\n",
    "            random_state=42,\n",
    "            stratify=common_patient_diagnoses.groupby('patient_id')['diagnosis'].first().values\n",
    "        )\n",
    "\n",
    "        # Add rare diagnosis patients to training set only\n",
    "        patient_train = np.concatenate([patient_train, rare_patients])\n",
    "    else:\n",
    "        # If there are no common diagnoses (unlikely), just do a regular split\n",
    "        patient_train, patient_test = train_test_split(\n",
    "            patient_groups['patient_id'].unique(),\n",
    "            test_size=0.2,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "    # Create masks for the original dataframe\n",
    "    train_mask = annotations_df['patient_id'].isin(patient_train)\n",
    "    test_mask = annotations_df['patient_id'].isin(patient_test)\n",
    "\n",
    "    train_df = annotations_df[train_mask]\n",
    "    test_df = annotations_df[test_mask]\n",
    "\n",
    "    print(f\"Training set: {len(train_df)} files from {len(patient_train)} patients\")\n",
    "    print(f\"Test set: {len(test_df)} files from {len(patient_test)} patients\")\n",
    "\n",
    "    # Verify class distribution\n",
    "    print(\"\\nDiagnosis distribution in training set:\")\n",
    "    print(train_df['diagnosis'].value_counts())\n",
    "    print(\"\\nDiagnosis distribution in test set:\")\n",
    "    print(test_df['diagnosis'].value_counts())\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def build_model(input_shape, num_classes):\n",
    "    \"\"\"Build a CNN model for audio classification\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot training and validation metrics\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "    # Plot accuracy\n",
    "    ax1.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    ax1.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    ax1.set_title('Model Accuracy')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    # Plot loss\n",
    "    ax2.plot(history.history['loss'], label='Training Loss')\n",
    "    ax2.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    ax2.set_title('Model Loss')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    cm = confusion_matrix(np.argmax(y_true, axis=1), np.argmax(y_pred, axis=1))\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def visualize_samples(X, y, filenames, patient_ids, class_names, n_samples=3):\n",
    "    \"\"\"Visualize some sample spectrograms from the dataset\"\"\"\n",
    "    plt.figure(figsize=(15, n_samples * 3))\n",
    "\n",
    "    # Get indices for each class\n",
    "    class_indices = {}\n",
    "    y_classes = np.argmax(y, axis=1)\n",
    "\n",
    "    for i, class_idx in enumerate(range(len(class_names))):\n",
    "        indices = np.where(y_classes == class_idx)[0]\n",
    "        if len(indices) > 0:\n",
    "            class_indices[class_idx] = indices\n",
    "\n",
    "    # Plot spectrograms for each class\n",
    "    plot_idx = 1\n",
    "    for class_idx, indices in class_indices.items():\n",
    "        # Pick n_samples random samples from this class\n",
    "        sample_indices = np.random.choice(indices, size=min(n_samples, len(indices)), replace=False)\n",
    "\n",
    "        for idx in sample_indices:\n",
    "            plt.subplot(len(class_indices), n_samples, plot_idx)\n",
    "            plt.imshow(X[idx, :, :, 0], aspect='auto', origin='lower', cmap='viridis')\n",
    "            plt.title(f\"Class: {class_names[class_idx]}\\nPatient: {patient_ids[idx]}\")\n",
    "            plt.tight_layout()\n",
    "            plot_idx += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "def main():\n",
    "    # Load diagnosis data from CSV\n",
    "    diagnosis_dict = load_diagnosis_data()\n",
    "    if not diagnosis_dict:\n",
    "        print(\"Could not load diagnosis data. Please check the CSV file path.\")\n",
    "        return\n",
    "\n",
    "    # Find all WAV files\n",
    "    wav_files = find_wav_files()\n",
    "    if not wav_files:\n",
    "        print(\"No WAV files found in the dataset directory.\")\n",
    "        return\n",
    "\n",
    "    # Build dataset by matching WAV files with diagnosis information\n",
    "    annotations_df = build_dataset(wav_files, diagnosis_dict)\n",
    "    if annotations_df is None or len(annotations_df) == 0:\n",
    "        print(\"Could not match WAV files with diagnosis information.\")\n",
    "        return\n",
    "\n",
    "    # Split data by patient to avoid data leakage\n",
    "    train_df, test_df = ensure_patient_stratification(annotations_df)\n",
    "\n",
    "    # Prepare training data\n",
    "    print(\"\\nPreparing training dataset...\")\n",
    "    X_train, y_train, train_filenames, train_patient_ids = prepare_dataset(train_df)\n",
    "\n",
    "    # Prepare test data\n",
    "    print(\"\\nPreparing test dataset...\")\n",
    "    X_test, y_test, test_filenames, test_patient_ids = prepare_dataset(test_df)\n",
    "\n",
    "    print(f\"Training set shape: X: {X_train.shape}, y: {y_train.shape}\")\n",
    "    print(f\"Test set shape: X: {X_test.shape}, y: {y_test.shape}\")\n",
    "\n",
    "    # Get class names for visualization\n",
    "    class_names = [k for k, v in sorted(class_mapping.items(), key=lambda item: item[1])]\n",
    "    class_names = list(dict.fromkeys(class_names))  # Remove duplicates while preserving order\n",
    "\n",
    "    # Visualize some samples\n",
    "    print(\"\\nVisualizing sample spectrograms from training set:\")\n",
    "    visualize_samples(X_train, y_train, train_filenames, train_patient_ids, class_names)\n",
    "\n",
    "    # Build and train model\n",
    "    input_shape = X_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "    print(f\"\\nBuilding model for {num_classes} classes...\")\n",
    "    model = build_model(input_shape, num_classes)\n",
    "    model.summary()\n",
    "\n",
    "    print(\"\\nTraining model...\")\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        callbacks=[early_stopping]\n",
    "    )\n",
    "\n",
    "    # Evaluate model\n",
    "    print(\"\\nEvaluating model...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    plot_training_history(history)\n",
    "    plot_confusion_matrix(y_test, y_pred, class_names)\n",
    "\n",
    "    y_true_classes = np.argmax(y_test, axis=1)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "    # Save model\n",
    "    model.save('/content/icbhi_diagnosis_model.h5')\n",
    "    print(\"\\nModel saved to /content/icbhi_diagnosis_model.h5\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531daa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tempfile\n",
    "\n",
    "# Define constants for audio processing\n",
    "SAMPLE_RATE = 22050\n",
    "DURATION = 5  # seconds\n",
    "N_MELS = 128\n",
    "HOP_LENGTH = 512\n",
    "\n",
    "# Define class mapping\n",
    "class_mapping = {\n",
    "    0: \"Healthy\",\n",
    "    1: \"COPD/Asthma/Bronchiectasis/Bronchiolitis\",\n",
    "    2: \"LRTI/Pneumonia\",\n",
    "    3: \"URTI\"\n",
    "}\n",
    "\n",
    "# Function to extract features from an audio file\n",
    "def extract_features(file_path):\n",
    "    try:\n",
    "        y, sr = librosa.load(file_path, sr=SAMPLE_RATE)\n",
    "\n",
    "        if len(y) > SAMPLE_RATE * DURATION:\n",
    "            y = y[:int(SAMPLE_RATE * DURATION)]\n",
    "        else:\n",
    "            pad_length = int(SAMPLE_RATE * DURATION) - len(y)\n",
    "            if pad_length > 0:\n",
    "                y = np.pad(y, (0, pad_length), mode='constant')\n",
    "\n",
    "        mel_spec = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS, hop_length=HOP_LENGTH)\n",
    "        mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "        mel_spec_db = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min())\n",
    "\n",
    "        return mel_spec_db\n",
    "    except Exception as e:\n",
    "        st.error(f\"Error processing audio: {e}\")\n",
    "        return None\n",
    "\n",
    "# Function to plot spectrogram\n",
    "def plot_spectrogram(spec, title=\"Mel Spectrogram\"):\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    img = librosa.display.specshow(spec, x_axis='time', y_axis='mel', sr=SAMPLE_RATE, hop_length=HOP_LENGTH, ax=ax)\n",
    "    ax.set_title(title)\n",
    "    fig.colorbar(img, ax=ax, format='%+2.0f dB')\n",
    "    return fig\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model = tf.keras.models.load_model('icbhi_diagnosis_model.h5')\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    st.title(\"Detecting Respiratory Diseases\")\n",
    "    st.write(\"\"\"\n",
    "    ## Upload a respiratory sound recording to classify the condition\n",
    "    This application uses a deep learning model trained on the ICBHI dataset to predict respiratory conditions.\n",
    "    \"\"\")\n",
    "\n",
    "    # Load model\n",
    "    try:\n",
    "        model = load_model()\n",
    "        st.success(\"Model loaded successfully!\")\n",
    "    except Exception as e:\n",
    "        st.error(f\"Failed to load model: {e}\")\n",
    "        return\n",
    "\n",
    "    # File uploader\n",
    "    uploaded_file = st.file_uploader(\"Upload a WAV file\", type=['wav'])\n",
    "\n",
    "    if uploaded_file is not None:\n",
    "        temp_dir = tempfile.TemporaryDirectory()\n",
    "        temp_file_path = os.path.join(temp_dir.name, \"temp_audio.wav\")\n",
    "\n",
    "        with open(temp_file_path, \"wb\") as f:\n",
    "            f.write(uploaded_file.getbuffer())\n",
    "\n",
    "        st.audio(uploaded_file, format='audio/wav')\n",
    "\n",
    "        with st.spinner('Processing audio...'):\n",
    "            features = extract_features(temp_file_path)\n",
    "\n",
    "            if features is not None:\n",
    "                st.subheader(\"Mel Spectrogram\")\n",
    "                fig = plot_spectrogram(features)\n",
    "                st.pyplot(fig)\n",
    "\n",
    "                features = features.reshape(1, features.shape[0], features.shape[1], 1)\n",
    "\n",
    "                with st.spinner('Classifying...'):\n",
    "                    prediction = model.predict(features)\n",
    "                    predicted_class = np.argmax(prediction[0])\n",
    "                    prediction_proba = prediction[0][predicted_class] * 100\n",
    "\n",
    "                st.subheader(\"Prediction Results\")\n",
    "                st.write(f\"**Predicted Condition:** {class_mapping[predicted_class]}\")\n",
    "                st.write(f\"**Confidence:** {prediction_proba:.2f}%\")\n",
    "\n",
    "                st.subheader(\"Class Probabilities\")\n",
    "                for class_id, class_name in class_mapping.items():\n",
    "                    st.write(f\"{class_name}: {prediction[0][class_id]*100:.2f}%\")\n",
    "\n",
    "                temp_dir.cleanup()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9219ccd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up ngrok\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Set your auth token - REPLACE WITH YOUR ACTUAL TOKEN\n",
    "ngrok.set_auth_token(\"2v8XsTmzMl5e639nypEY1cVJyZr_3nfL3ZpPLiPp5x2RKhjuc\")\n",
    "\n",
    "# Terminate any existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Run the Streamlit app\n",
    "!streamlit run app.py &>/dev/null &\n",
    "\n",
    "# Create a tunnel to the Streamlit port\n",
    "public_url = ngrok.connect(8501)\n",
    "print(f\"Public URL: {public_url}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
